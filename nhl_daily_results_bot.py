#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
NHL Daily Results -> Telegram (RU) via championat.com

–ò—Å—Ç–æ—á–Ω–∏–∫:
  - –ö–∞–ª–µ–Ω–¥–∞—Ä—å: https://www.championat.com/hockey/_nhl/tournament/6606/calendar/
  - –°—Ç—Ä–∞–Ω–∏—Ü–∞ –º–∞—Ç—á–∞: /hockey/_nhl/xxxxxx/match/zzzzzz.html (–≤–Ω—É—Ç—Ä–∏ ‚Äî –≥–æ–ª—ã, –∞—Å—Å–∏—Å—Ç—ã, –±—É–ª–ª–∏—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º)

–ü—Ä–∞–≤–∏–ª–∞ –≤–∫–ª—é—á–µ–Ω–∏—è –º–∞—Ç—á–µ–π:
  - –î–ª—è –ø–æ—Å—Ç–∞ –æ—Ç –¥–∞—Ç—ã D (–ø–æ –ú–°–ö) –±–µ—Ä—ë–º:
      * –≤—Å–µ –º–∞—Ç—á–∏ —Å –¥–∞—Ç–æ–π –Ω–∞—á–∞–ª–∞ = D (–ú–°–ö),
      * –≤—Å–µ –º–∞—Ç—á–∏ —Å –¥–∞—Ç–æ–π –Ω–∞—á–∞–ª–∞ = D-1, —É –∫–æ—Ç–æ—Ä—ã—Ö –≤—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞ >= 15:00 –ú–°–ö.
–§–æ—Ä–º–∞—Ç —Å–æ–±—ã—Ç–∏–π:
  - ¬´–°—á—ë—Ç ‚Äì MM.SS –ê–≤—Ç–æ—Ä (–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç1, –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç2)¬ª
  - –í—Ä–µ–º—è ‚Äî –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –æ—Ç —Å—Ç–∞—Ä—Ç–∞ –º–∞—Ç—á–∞ (1-–π –ø–µ—Ä–∏–æ–¥ 0‚Äì20, 2-–π 20‚Äì40, 3-–π 40‚Äì60, –û–¢ ‚Äî 60+)
  - –°–µ—Ä–∏—è –±—É–ª–ª–∏—Ç–æ–≤: —Ç–æ–ª—å–∫–æ ¬´–ü–æ–±–µ–¥–Ω—ã–π –±—É–ª–ª–∏—Ç¬ª –∏ –∞–≤—Ç–æ—Ä.

–¢—Ä–µ–±—É—é—Ç—Å—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:
  - TELEGRAM_BOT_TOKEN
  - TELEGRAM_CHAT_ID
–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ:
  - REPORT_DATE=YYYY-MM-DD  (–¥–∞—Ç–∞ –ø–æ—Å—Ç–∞ –ø–æ –ú–°–ö)

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
  requests==2.32.3
  beautifulsoup4==4.12.3
"""

import os, re, sys, time, json, random, datetime as dt
from zoneinfo import ZoneInfo
from html import escape

import requests
from bs4 import BeautifulSoup

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã/–∫–æ–Ω—Ñ–∏–≥
CAL_URL = "https://www.championat.com/hockey/_nhl/tournament/6606/calendar/"
BASE    = "https://www.championat.com"
TZ_MSK  = ZoneInfo("Europe/Moscow")
TZ_CHAT = TZ_MSK  # –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–∞—Ç —Ç–æ–∂–µ –ø–æ –ú–°–ö

BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
CHAT_ID   = os.getenv("TELEGRAM_CHAT_ID", "").strip()
REPORT_DATE_ENV = os.getenv("REPORT_DATE", "").strip()

USER_AGENT = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
              "AppleWebKit/537.36 (KHTML, like Gecko) "
              "Chrome/125.0 Safari/537.36")

JITTER = (0.4, 0.9)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ HTTP
S = requests.Session()
S.headers.update({
    "User-Agent": USER_AGENT,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
    "Connection": "keep-alive",
})

def jitter():
    time.sleep(random.uniform(*JITTER))

def get_html(url: str) -> str:
    try:
        r = S.get(url, timeout=25)
        if r.status_code == 200:
            return r.text
        # –∏–Ω–æ–≥–¥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤–µ—à–∞–µ—Ç—Å—è ‚Äî –ø–æ–≤—Ç–æ—Ä
        if r.status_code in (403, 429, 500, 502, 503, 504):
            jitter()
            r2 = S.get(url, timeout=25)
            if r2.status_code == 200:
                return r2.text
        print(f"[WARN] GET {url} -> {r.status_code}", file=sys.stderr)
    except Exception as e:
        print(f"[ERR ] GET {url} -> {e}", file=sys.stderr)
    return ""

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –î–∞—Ç—ã/—Ç–µ–∫—Å—Ç
RU_MONTHS = {
    1:"—è–Ω–≤–∞—Ä—è",2:"—Ñ–µ–≤—Ä–∞–ª—è",3:"–º–∞—Ä—Ç–∞",4:"–∞–ø—Ä–µ–ª—è",5:"–º–∞—è",6:"–∏—é–Ω—è",
    7:"–∏—é–ª—è",8:"–∞–≤–≥—É—Å—Ç–∞",9:"—Å–µ–Ω—Ç—è–±—Ä—è",10:"–æ–∫—Ç—è–±—Ä—è",11:"–Ω–æ—è–±—Ä—è",12:"–¥–µ–∫–∞–±—Ä—è"
}
def ru_date(d: dt.date) -> str:
    return f"{d.day} {RU_MONTHS[d.month]}"

def pick_report_date() -> dt.date:
    if REPORT_DATE_ENV:
        try:
            return dt.date.fromisoformat(REPORT_DATE_ENV)
        except Exception:
            pass
    now = dt.datetime.now(TZ_MSK)
    return now.date()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –£—Ç–∏–ª–∏—Ç—ã —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def initial_ru(full_ru: str) -> str:
    """
    ¬´–ò–º—è –§–∞–º–∏–ª–∏—è¬ª/¬´–ò–º—è-–ò–º—è –§–∞–º–∏–ª–∏—è-–§–∞–º–∏–ª–∏—è¬ª -> ¬´–ò. –§–∞–º–∏–ª–∏—è-–§–∞–º–∏–ª–∏—è¬ª
    –ï—Å–ª–∏ —Ñ–∞–º–∏–ª–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–∞—è ‚Äî –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω (—Å –¥–µ—Ñ–∏—Å–∞–º–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º).
    """
    t = (full_ru or "").strip()
    if not t:
        return ""
    # —É–±–µ—Ä—ë–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    t = re.sub(r"\s+", " ", t)
    parts = t.split(" ")
    if len(parts) == 1:
        # –±—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ñ–∞–º–∏–ª–∏—è
        fam = parts[0]
        return fam
    ini = parts[0][0] + "."
    fam = parts[-1]
    return f"{ini} {fam}"

def abs_time_from_period(period: int, mmss: str) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ä–µ–º—è –ø–µ—Ä–∏–æ–¥–∞ (MM:SS) –≤ –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ MM.SS (1–ø=0‚Äì20, 2–ø=20‚Äì40...)"""
    m = re.match(r"^\s*(\d{1,2})[:.](\d{2})\s*$", mmss)
    if not m:
        return mmss.replace(":", ".")
    mm, ss = int(m.group(1)), int(m.group(2))
    if period >= 4:  # OT
        return f"{60 + mm}.{ss:02d}"
    return f"{(period-1)*20 + mm}.{ss:02d}"

def bold_winner_line(name: str, score: int, winner: bool) -> str:
    s = f"{escape(name)}: {score}"
    return f"<b>{s}</b>" if winner else s

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–ª–µ–Ω–¥–∞—Ä—è ‚áí —Å—Å—ã–ª–∫–∏ –º–∞—Ç—á–µ–π
def find_match_links_for_date_range(cal_html: str, dates_keep: set[dt.date]) -> list[str]:
    """
    –ò–∑ HTML –∫–∞–ª–µ–Ω–¥–∞—Ä—è –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –º–∞—Ç—á–µ–π '/hockey/_nhl/.../match/....html'
    –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞–µ—Ç –∏—Ö –ø–æ –¥–∞—Ç–∞–º –Ω–∞—á–∞–ª–∞ –∏–∑ —Å–∞–º–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–∞—Ç—á–∞ (–ø–æ –ú–°–ö).
    –ß—Ç–æ–±—ã –Ω–µ –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞–ª–µ–Ω–¥–∞—Ä—è, –≤—Ä–µ–º—è –º–∞—Ç—á–∞ –±–µ—Ä—ë–º —Å match-—Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    soup = BeautifulSoup(cal_html, "html.parser")
    # –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–∞—Ç—á–∏
    links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "/match/" in href and href.endswith(".html"):
            if not href.startswith("http"):
                href = BASE + href
            links.append(href)
    # —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º
    seen, uniq = set(), []
    for u in links:
        if u not in seen:
            uniq.append(u); seen.add(u)

    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ —Å—Ç–∞—Ä—Ç–∞ (–¥–æ—Å—Ç–∞—ë–º –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–∞—Ç—á–∞)
    out = []
    for url in uniq:
        d_start = get_match_start_date_msk(url)
        if not d_start:
            # –µ—Å–ª–∏ –¥–∞—Ç—É –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∏, –æ—Å—Ç–∞–≤–∏–º ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–Ω—ã–π –º–∞—Ç—á; –ª–∏—à–Ω–µ–µ –æ—Ç—Å–µ–µ–º –ø–æ–∑–∂–µ
            out.append(url)
        elif d_start in dates_keep:
            out.append(url)
    return out

def get_match_start_date_msk(match_url: str) -> dt.date | None:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –¥–æ—Å—Ç–∞—Ç—å –¥–∞—Ç—É –Ω–∞—á–∞–ª–∞ –º–∞—Ç—á–∞ –ø–æ –ú–°–ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–∞—Ç—á–∞.
    –ò—â–µ–º <time datetime="YYYY-MM-DDTHH:MM:SS+03:00">, –ª–∏–±–æ —Ç–µ–∫—Å—Ç –≤–∏–¥–∞ '–ù–∞—á–∞–ª–æ: 02:30 ... 27 –æ–∫—Ç—è–±—Ä—è ...'
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–∞—Ç—É (–ø–æ –ú–°–ö).
    """
    html = get_html(match_url)
    if not html:
        return None
    soup = BeautifulSoup(html, "html.parser")

    # –ü–æ–ø—É–ª—è—Ä–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç ‚Äî <time datetime="...+03:00">
    for t in soup.find_all("time"):
        dt_attr = t.get("datetime") or t.get("content")
        if dt_attr:
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å ISO —Å —Ç–∞–π–º–∑–æ–Ω–æ–π
                dt_full = dt.datetime.fromisoformat(dt_attr.replace("Z", "+00:00"))
                # –ü–µ—Ä–µ–≤–µ–¥—ë–º –≤ –ú–°–ö
                d_msk = dt_full.astimezone(TZ_MSK).date()
                return d_msk
            except Exception:
                pass

    # –§–æ–ª–±—ç–∫: –∏—â–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é –¥–∞—Ç—É –≤ —à–∞–ø–∫–µ
    full_text = soup.get_text("\n", strip=True)
    m = re.search(r"(\d{4})-(\d{2})-(\d{2})[ T](\d{2}):(\d{2})", full_text)
    if m:
        y, mo, d, hh, mm = map(int, m.groups())
        try:
            dtt = dt.datetime(y, mo, d, hh, mm, tzinfo=TZ_MSK)
            return dtt.date()
        except Exception:
            pass

    # –ï—Å–ª–∏ —Å–æ–≤—Å–µ–º –Ω–µ –Ω–∞—à–ª–∏ ‚Äî None
    return None

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–∞—Ç—á–∞
GOAL_LINE_RE = re.compile(
    r"(?P<score>\d+:\d+)\s*[‚Äî‚Äì-]\s*(?P<time>\d{1,2}[:.]\d{2})\s+(?P<who>[–ê-–Ø–ÅA-Z][^()\n\r]*?)(?:\s*\((?P<ass>[^)]*)\))?(?=\s|$)",
    re.U
)
PERIOD_RE_LIST = [
    (re.compile(r"\b1[-‚Äì]?–π\s+–ø–µ—Ä–∏–æ–¥\b", re.I | re.U), 1),
    (re.compile(r"\b2[-‚Äì]?–π\s+–ø–µ—Ä–∏–æ–¥\b", re.I | re.U), 2),
    (re.compile(r"\b3[-‚Äì]?–π\s+–ø–µ—Ä–∏–æ–¥\b", re.I | re.U), 3),
    (re.compile(r"\b–û–≤–µ—Ä—Ç–∞–π–º(?:\s*‚Ññ\s*(\d+))?\b", re.I | re.U), 4),  # –±–∞–∑–æ–≤–æ 4, –µ—Å–ª–∏ ‚ÑñN ‚Üí 3+N
]

def parse_match_page(match_url: str) -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      {
        "home": "–¢–∞–º–ø–∞-–ë—ç–π", "away": "–ê–Ω–∞—Ö–∞–π–º",
        "home_score": 4, "away_score": 3,
        "ot": False, "so": True/False,
        "goals": [ {"score":"1:0","abs_time":"9.10","scorer":"–î. –ì–µ–Ω—Ç—Ü–µ–ª—å","ass":["–ë. –•—ç–π–≥–µ–ª","–ê. –ß–∏—Ä–µ–ª–ª–∏"]}, ... ],
        "so_winner": "–¢. –ó–µ–≥—Ä–∞—Å" | None
      }
    """
    html = get_html(match_url)
    if not html:
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –º–∞—Ç—á–∞: {match_url}")
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text("\n", strip=True)
    text = re.sub(r"[ \t]+", " ", text)
    text = text.replace("‚Äî", "‚Äì").replace("‚àí", "‚Äì").replace("‚Äí", "‚Äì")

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫: –∫–æ–º–∞–Ω–¥—ã –∏ –∏—Ç–æ–≥–æ–≤—ã–π —Å—á—ë—Ç
    # –ü—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å –∏–∑ <h1>
    h1 = soup.find("h1")
    h1txt = h1.get_text(" ", strip=True) if h1 else ""
    # –ß–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–π—Å—è —Ñ–æ—Ä–º–∞—Ç: ¬´–¢–∞–º–ø–∞-–ë—ç–π¬ª ‚Äì ¬´–ê–Ω–∞—Ö–∞–π–º¬ª 4:3 (..)
    m_hdr = re.search(r"¬´?([–ê-–Ø–ÅA-Za-z \-\.]+?)¬ª?\s*[‚Äì-]\s*¬´?([–ê-–Ø–ÅA-Za-z \-\.]+?)¬ª?\s+(\d+):(\d+)(?:\s*\((.*?)\))?\s*(\(?(–û–¢|–ë)\)?)?", h1txt)
    if not m_hdr:
        # —Ñ–æ–ª–±—ç–∫ ‚Äî –∏—Å–∫–∞—Ç—å –ø–æ—Ö–æ–∂–µ–µ –≤ —Ç–µ–∫—Å—Ç–µ
        m_hdr = re.search(r"([–ê-–Ø–ÅA-Za-z \-\.]+?)\s*[‚Äì-]\s*([–ê-–Ø–ÅA-Za-z \-\.]+?)\s+(\d+):(\d+)", text)
    if not m_hdr:
        raise RuntimeError(f"–ù–µ —Ä–∞–∑–æ–±—Ä–∞–ª –∑–∞–≥–æ–ª–æ–≤–æ–∫/—Å—á—ë—Ç: {match_url}")

    home = m_hdr.group(1).strip().strip("¬´¬ª")
    away = m_hdr.group(2).strip().strip("¬´¬ª")
    home_score = int(m_hdr.group(3))
    away_score = int(m_hdr.group(4))
    winner_home = home_score > away_score

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –±—ã–ª –ª–∏ –û–¢/–ë
    so = bool(re.search(r"\b–ë—É–ª–ª–∏—Ç—ã\b|\(–ë\)|–≤ —Å–µ—Ä–∏–∏ –±—É–ª–ª–∏—Ç–æ–≤", text, re.I))
    ot = bool(re.search(r"\b–û–≤–µ—Ä—Ç–∞–π–º\b|\(–û–¢\)", text, re.I))

    # –í—ã–¥–µ–ª—è–µ–º —Å–µ–∫—Ü–∏—é —Å –≥–æ–ª–∞–º–∏ (–º–µ–∂–¥—É –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ —Ä–∞–∑–¥–µ–ª–æ–≤)
    # –ò—â–µ–º –Ω–∞—á–∞–ª–æ –ø–æ ¬´1-–π –ø–µ—Ä–∏–æ–¥¬ª/¬´–ì–æ–ª—ã¬ª –∏ –∫–æ–Ω–µ—Ü –ø–æ ¬´–£–¥–∞–ª–µ–Ω–∏—è|–®—Ç—Ä–∞—Ñ|–ë—É–ª–ª–∏—Ç—ã|–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞¬ª –∏ —Ç.–ø.
    start_idx = None
    for m in re.finditer(r"(1[-‚Äì]?–π\s+–ø–µ—Ä–∏–æ–¥|–ì–æ–ª—ã|–•–æ–¥ –º–∞—Ç—á–∞)", text, re.I):
        start_idx = m.start(); break
    if start_idx is None:
        # –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ ‚Äî —Å –Ω–∞—á–∞–ª–∞ h1 –≤–Ω–∏–∑
        start_idx = text.find(h1txt) + len(h1txt) if h1txt else 0
    end_m = re.search(r"(–£–¥–∞–ª–µ–Ω–∏—è|–®—Ç—Ä–∞—Ñ|–ë—É–ª–ª–∏—Ç—ã|–°–µ—Ä–∏—è –±—É–ª–ª–∏—Ç–æ–≤|–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞|–°–æ—Å—Ç–∞–≤—ã|–í–∏–¥–µ–æ)", text, re.I)
    end_idx = end_m.start() if end_m else len(text)

    goals_section = text[start_idx:end_idx]

    # –ü–∞—Ä—Å–∏–º –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º
    period = 1
    goals = []
    # –†–∞–∑–æ–±—å—ë–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–µ—Ä–∏–æ–¥–æ–≤
    lines = [ln.strip() for ln in goals_section.split("\n") if ln.strip()]
    for ln in lines:
        # –î–µ—Ç–µ–∫—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø–µ—Ä–∏–æ–¥–∞
        hit_period = False
        for rx, base_p in PERIOD_RE_LIST:
            m = rx.search(ln)
            if m:
                if base_p == 4 and m.lastindex == 1 and m.group(1):
                    # ¬´–û–≤–µ—Ä—Ç–∞–π–º ‚ÑñN¬ª
                    try:
                        n = int(m.group(1))
                    except Exception:
                        n = 1
                    period = 3 + max(1, n)
                else:
                    period = base_p
                hit_period = True
                break
        if hit_period:
            continue

        # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–æ–ª–æ–≤
        for m in GOAL_LINE_RE.finditer(ln):
            score = m.group("score").strip()
            t_in  = m.group("time").replace(".", ":")
            who   = m.group("who").strip()
            ass   = (m.group("ass") or "").strip()

            # —á–∏—Å—Ç–∏–º –∏–º—è/–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ (non-breaking –∏ –ª–∞—Ç–∏–Ω–∏—Ü—É)
            who = re.sub(r"\s+", " ", who)
            # –ò–Ω–æ–≥–¥–∞ –ø–æ—Å–ª–µ –∏–º–µ–Ω–∏ –¥–æ–±–∞–≤–ª—è—é—Ç –ª–∏—à–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–∏–ø–∞ ¬´‚Äî –∫–æ–º–∞–Ω–¥–∞¬ª, –æ—Ç—Ä–µ–∂–µ–º –ø–æ ¬´ ‚Äì ¬ª –∏–ª–∏ ¬´ ‚Äî ¬ª
            who = re.split(r"\s+[‚Äì-]\s+", who)[0].strip()

            # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ ¬´–ò. –§–∞–º–∏–ª–∏—è¬ª
            who_sh = initial_ru(who)

            assists = []
            if ass:
                for a in ass.split(","):
                    aa = a.strip()
                    # –æ—Ç—Ä–µ–∑–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ ¬´‚Äî –∫–æ–º–∞–Ω–¥–∞¬ª
                    aa = re.split(r"\s+[‚Äì-]\s+", aa)[0].strip()
                    if aa:
                        assists.append(initial_ru(aa))

            abs_t = abs_time_from_period(period, t_in)
            goals.append({
                "score": score,
                "abs_time": abs_t,
                "scorer": who_sh,
                "ass": assists
            })

    # –ü–æ–±–µ–¥–Ω—ã–π –±—É–ª–ª–∏—Ç
    so_winner = None
    if so:
        # –Ø–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞
        m = re.search(r"–ü–æ–±–µ–¥–Ω—ã–π\s+–±—É–ª–ª–∏—Ç[:\s‚Äì-]+([–ê-–Ø–ÅA-Z][^,\n\r]+)", text, re.I)
        if m:
            so_winner = initial_ru(m.group(1).strip())
        else:
            # –ò–Ω–æ–≥–¥–∞ –≤ –±–ª–æ–∫–µ –±—É–ª–ª–∏—Ç–æ–≤ –æ—Ç–º–µ—á–∞—é—Ç ¬´—Ä–µ—à–∞—é—â–∏–π¬ª ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º
            m2 = re.search(r"(—Ä–µ—à–∞—é—â|–ø–æ–±–µ–¥–Ω)[^:\n\r]*[:\s‚Äì-]+([–ê-–Ø–ÅA-Z][^,\n\r]+)", text, re.I)
            if m2:
                so_winner = initial_ru(m2.group(2).strip())

    return {
        "home": home, "away": away,
        "home_score": home_score, "away_score": away_score,
        "winner_home": winner_home,
        "ot": ot, "so": so,
        "goals": goals,
        "so_winner": so_winner,
        "url": match_url
    }

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –°–±–æ—Ä –º–∞—Ç—á–µ–π –ø–æ –ø—Ä–∞–≤–∏–ª—É ¬´–¥–µ–Ω—å D + –¥–µ–Ω—å D-1 –ø–æ—Å–ª–µ 15:00¬ª
def collect_match_urls_for_report(d_report: dt.date) -> list[str]:
    """
    –ì—Ä—É–∑–∏–º –∫–∞–ª–µ–Ω–¥–∞—Ä—å (–æ—Å–Ω–æ–≤–Ω–æ–π), –¥–æ—Å—Ç–∞—ë–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –º–∞—Ç—á–µ–π, —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–∞–º –Ω–∞—á–∞–ª–∞:
      - –¥–∞—Ç–∞ == D
      - –¥–∞—Ç–∞ == D-1 –∏ –≤—Ä–µ–º—è >= 15:00 –ú–°–ö
    –ß—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —Ç–æ–≥–æ, –æ—Ç–¥–∞—ë—Ç –ª–∏ –∫–∞–ª–µ–Ω–¥–∞—Ä—å —Å—Ä–∞–∑—É –æ–±–µ –¥–∞—Ç—ã, –ø–æ–¥—Å—Ç—Ä–∞—Ö—É–µ–º—Å—è:
      - —Å–Ω–∞—á–∞–ª–∞ –±–µ—Ä—ë–º –±–∞–∑–æ–≤—ã–π –∫–∞–ª–µ–Ω–¥–∞—Ä—å,
      - –µ—Å–ª–∏ D-1 –¥—Ä—É–≥–æ–π –º–µ—Å—è—Ü, –ø—Ä–æ–±—É–µ–º –µ—â—ë —Ä–∞–∑ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º ?date=YYYY-MM-01
    """
    html_main = get_html(CAL_URL)
    if not html_main:
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–ª–µ–Ω–¥–∞—Ä—å Championat")

    # –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–º—ã –≤—Å—ë —Ä–∞–≤–Ω–æ —Å–≤–µ—Ä–∏–º –¥–∞—Ç—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –º–∞—Ç—á–∞)
    cand = []
    # –ò–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—è
    cand += find_match_links_for_date_range(html_main, {d_report, d_report - dt.timedelta(days=1)})

    # –ï—Å–ª–∏ D-1 –≤ –¥—Ä—É–≥–æ–º –º–µ—Å—è—Ü–µ ‚Äî –ø–æ–¥—Å—Ç—Ä–∞—Ö—É–µ–º—Å—è
    d_prev = d_report - dt.timedelta(days=1)
    if d_prev.month != d_report.month:
        url_prev_month = CAL_URL + f"?date={d_prev.strftime('%Y-%m-01')}"
        html_prev = get_html(url_prev_month)
        if html_prev:
            cand += find_match_links_for_date_range(html_prev, {d_prev})

    # –£–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º
    seen, uniq = set(), []
    for u in cand:
        if u not in seen:
            uniq.append(u); seen.add(u)

    # –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–æ—á–Ω–æ–º—É –ø—Ä–∞–≤–∏–ª—É –≤—Ä–µ–º–µ–Ω–∏
    selected = []
    for url in uniq:
        # –ø–æ–ø—Ä–æ–±—É–µ–º –¥–æ—Å—Ç–∞—Ç—å —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞ –∏–∑ <time datetime=...>
        html = get_html(url)
        if not html:
            continue
        soup = BeautifulSoup(html, "html.parser")
        start_dt = None
        for t in soup.find_all("time"):
            dt_attr = t.get("datetime") or t.get("content")
            if dt_attr:
                try:
                    dtt = dt.datetime.fromisoformat(dt_attr.replace("Z", "+00:00"))
                    start_dt = dtt.astimezone(TZ_MSK)
                    break
                except Exception:
                    pass
        # —Ñ–æ–ª–±—ç–∫ ‚Äî –Ω–µ –Ω–∞—à–ª–∏ <time> ‚Üí –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–æ –¥–∞—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
        if not start_dt:
            donly = get_match_start_date_msk(url)
            if not donly:
                continue
            # –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –≤—Ä–µ–º—è ‚Äî –ø—É—Å—Ç—å —Ä–µ—à–∏—Ç –ø–æ –¥–∞—Ç–µ
            if donly == d_report:
                selected.append(url)
            elif donly == d_prev:
                # –±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏ –Ω–µ –º–æ–∂–µ–º –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–ª—å—Ç—Ä 15:00 ‚Äî –ø—Ä–æ–ø—É—Å—Ç–∏–º
                continue
            continue

        d_local = start_dt.date()
        if d_local == d_report:
            selected.append(url)
        elif d_local == d_prev and start_dt.time() >= dt.time(15, 0):
            selected.append(url)

    return selected

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞
def build_post_text(d_report: dt.date) -> str:
    urls = collect_match_urls_for_report(d_report)
    # –ü–∞—Ä—Å–∏–º –≤—Å–µ –º–∞—Ç—á–∏
    matches = []
    for u in urls:
        try:
            jitter()
            matches.append(parse_match_page(u))
        except Exception as e:
            print(f"[WARN] {u}: {e}", file=sys.stderr)

    # –û—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–µ
    matches = [m for m in matches if m.get("goals") or m.get("so") or (m.get("home") and m.get("away"))]
    n = len(matches)

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title = f"üóì –†–µ–≥—É–ª—è—Ä–Ω—ã–π —á–µ–º–ø–∏–æ–Ω–∞—Ç –ù–•–õ ‚Ä¢ {ru_date(d_report)} ‚Ä¢ {n} " + \
            ("–º–∞—Ç—á" if n == 1 else "–º–∞—Ç—á–∞" if n % 10 in (2, 3, 4) and not 12 <= n % 100 <= 14 else "–º–∞—Ç—á–µ–π")
    head = f"{title}\n\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞–¥—ë–∂–Ω–æ —Å–ø—Ä—è—Ç–∞–Ω—ã üëá\n\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n\n"

    if not matches:
        return head.strip()

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞? –°–µ–π—á–∞—Å –ø–æ—Ä—è–¥–æ–∫ –∫–∞–ª–µ–Ω–¥–∞—Ä—è/—Å—Å—ã–ª–æ–∫.
    blocks = []
    for idx, m in enumerate(matches, 1):
        home_line = bold_winner_line(m["home"], m["home_score"], m["winner_home"])
        away_line = bold_winner_line(m["away"], m["away_score"], not m["winner_home"])

        suffix = ""
        if m["so"]:
            suffix = " (–ë)"
        elif m["ot"]:
            suffix = " (–û–¢)"

        lines = [f"{home_line}{suffix}", f"{away_line}"]

        # —Å–æ–±—ã—Ç–∏—è
        for ev in m["goals"]:
            ass = f" ({', '.join(escape(a) for a in ev['ass'])})" if ev["ass"] else ""
            lines.append(f"{escape(ev['score'])} ‚Äì {escape(ev['abs_time'])} {escape(ev['scorer'])}{ass}")

        if m["so"] and m.get("so_winner"):
            lines.append("<i>–ü–æ–±–µ–¥–Ω—ã–π –±—É–ª–ª–∏—Ç</i>")
            lines.append(f"65.00 {escape(m['so_winner'])}")

        blocks.append("\n".join(lines))
        if idx < len(matches):
            blocks.append("")

    return head + "\n".join(blocks).strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Telegram
def tg_send(text: str):
    if not (BOT_TOKEN and CHAT_ID):
        raise RuntimeError("TELEGRAM_BOT_TOKEN / TELEGRAM_CHAT_ID –Ω–µ –∑–∞–¥–∞–Ω—ã")
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    maxlen = 3500
    rest = text
    while rest:
        if len(rest) <= maxlen:
            chunk, rest = rest, ""
        else:
            cut = rest.rfind("\n\n", 0, maxlen)
            if cut == -1:
                cut = maxlen
            chunk, rest = rest[:cut], rest[cut:].lstrip()
        r = S.post(url, json={
            "chat_id": CHAT_ID,
            "text": chunk,
            "parse_mode": "HTML",
            "disable_web_page_preview": True
        }, timeout=25)
        if r.status_code != 200:
            raise RuntimeError(f"Telegram error {r.status_code}: {r.text[:200]}")
        time.sleep(0.25)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ main
if __name__ == "__main__":
    try:
        d = pick_report_date()
        post = build_post_text(d)
        tg_send(post)
        print("OK")
    except Exception as e:
        print("ERROR:", repr(e), file=sys.stderr)
        sys.exit(1)
